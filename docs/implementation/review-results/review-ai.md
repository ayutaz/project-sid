# AI・認知システム レビュー

## エグゼクティブサマリー

3ドキュメント（02-llm-integration.md、03-cognitive-controller.md、04-memory-system.md）は全体として**高品質**であり、論文のPIANOアーキテクチャを実装設計に落とし込む上で十分な深さと網羅性を持っている。特にCCの実装設計（03）と記憶システム（04）は、論文の記述を超えて認知科学の文献やGenerative Agents等の先行研究を適切に参照しており、実装の具体性が高い。

一方、以下の重要な問題が確認された：

1. **LLM価格情報の誤り**: Claude Haiku 4.5の価格が旧世代（Haiku 3.5）の値で記載されている
2. **Qdrantベンチマーク数値の過大評価**: ドキュメント記載の6ms/2,000 QPSは公式ベンチマークの実測値と大きく乖離
3. **情報ボトルネック理論の不完全な反映**: 論文のGWT情報ボトルネックは「設計者が情報の流れを明示的に制御」する概念だが、実装設計ではLLMプロンプトへの情報注入に矮小化されている面がある
4. **バッチ処理のリアルタイム性との矛盾**: OpenAI Batch APIの24時間返却をPIANOの低速モジュール（60秒間隔）に適用する設計に根本的な齟齬がある
5. **非LLMモジュールの設計深度不足**: 論文で「最も重要」とされる行動認識モジュールの設計が浅い

**総合評価**: B+（良好。上記の修正により実装着手可能な水準に到達する）

## レビュー対象
- 02-llm-integration.md（LLM統合戦略）
- 03-cognitive-controller.md（認知コントローラ実装方針）
- 04-memory-system.md（記憶システム設計）

---

## 詳細レビュー

### 1. 論文との整合性

#### 1.1 GWT（Global Workspace Theory）の反映

**良い点**:
- 03-cognitive-controller.md のGWT解説（セクション1）は正確で、Baars (1988)の原典を適切に参照している
- GWTの3フェーズ（競争→点火→ブロードキャスト）の計算モデルへの変換は概念的に正しい
- LIDAフレームワークからの知見の取り込み（BroadcastListener、注意コードレット等）は適切

**問題点**:
- **GWTの「意識へのアクセス競争」の実装が不十分**: 論文では各モジュールが情報をワークスペースに「投入」し、重要度に基づく「競争」が行われるとされる。しかし実装設計では、全モジュールの出力が単にCCのプロンプトに注入されるだけで、真の「競争」メカニズム（一部の情報のみがワークスペースを「占有」する仕組み）が明示的に設計されていない。セクション2.3の情報選択アルゴリズム（緊急性/関連性/新規性の3軸評価）はフィルタリングであり、GWTの「点火（Ignition）」プロセスに対応する「閾値を超えた情報のみが選択される」動的メカニズムとは異なる
- **LIDAの認知サイクル時間**: LIDAでは約300ms/サイクルとされているが、PIANO CCの中速（1-15秒）との対応関係について考察がない。生物学的認知サイクル（~300ms）とLLM処理時間（1-5秒）のスケール差をどう解釈するかの議論があるとよい

#### 1.2 情報ボトルネック理論の反映

**良い点**:
- 論文の「CCがエージェント状態から得られる情報を統合・圧縮」「提示される情報量を削減しつつ、設計者が情報の流れを明示的に制御」という記述を、圧縮アルゴリズム（フィルタリング→要約→テンプレート化）の3段階で具体化している
- 圧縮率と情報損失のトレードオフを定量的に示している（低/中/高圧縮の3レベル）

**問題点**:
- **情報理論的な「ボトルネック」の意味の希薄化**: Tishby et al.の情報ボトルネック理論では、入力Xと出力Yの間に圧縮変数Tを挟み、I(T;X)を最小化しつつI(T;Y)を最大化する。論文もこの理論を参照しているが、実装設計ではこの最適化的観点が欠落している。「何トークンに圧縮するか」ではなく「どの情報が意思決定（Y）に最も寄与するか」の観点での情報選択が望ましい
- 03のセクション2.6の情報提示フォーマットが固定的で、状況依存的な情報の重み付けが弱い

#### 1.3 ブロードキャスト機構の反映

**良い点**:
- 論文の「CCの意思決定を全モジュールに一斉配信」「発話関連の出力を強く条件付け」を、条件付け強度の階層（強0.9/中0.6/弱0.3）で具体化している
- 「言語的コミットメントと実際の行動の一貫性」を`speech_constraints`と`active_commitments`で明示的に管理している

**問題点なし**: ブロードキャスト機構の設計は論文の意図を忠実かつ具体的に反映している。

#### 1.4 記憶システムの三層構造

**良い点**:
- 論文のWM/STM/LTMの三層構造を忠実に反映
- 論文が「具体的な時間パラメータや容量制限は明記されていない」ことを正直に認め、認知科学の知見と実験規模から推定値を提案している

**問題点**:
- 論文では記憶モジュールの速度を「可変」としているが、04の設計ではWM更新（1-5秒）、STM統合（5-10分）、LTMメンテナンス（不定期）と比較的固定的。論文の「可変速度対応」をもう少し動的に設計してもよい

---

### 2. LLM統合設計

#### 2.1 API設計

**良い点**:
- 統一インターフェース（`LLMProvider`）の設計は堅実で、TypeScriptの型定義が明確
- モジュール別メタデータ（`PIANOModule`、`agentId`、`priority`）の付与により、ルーティング・コスト追跡が容易
- フォールバック戦略の5段階設計（プライマリ→同プロバイダ→クロスプロバイダ→ローカル→デグレード）は十分にロバスト

**問題点**:
- **TypeScriptとPythonの混在**: 02ではTypeScript、03-04ではPythonで設計されている。00-overview.mdでPython 3.12+が主要言語と決定されているため、02のインターフェースもPythonで統一すべき。あるいはTypeScriptフロントエンドとPythonバックエンドのブリッジ設計を明示すべき
- **ストリーミング応答の活用方針**: `streamComplete`メソッドが定義されているが、PIANOのどのモジュールでストリーミングを使うかの方針がない。特に発話モジュールではストリーミングにより体感レイテンシを大幅に改善できる可能性がある

#### 2.2 モデル選択

**良い点**:
- モジュールごとのTier分け（高性能/中性能/軽量/非LLM）は論文のモジュール速度区分と整合している
- 規模別のハイブリッド構成（10-50体: API中心、100-500体: ハイブリッド、500-1000体: ローカル中心）は実用的

**問題点（価格情報の誤り）**:

| モデル | ドキュメント記載 | 実際の価格（2026年2月時点） | 差異 |
|---|---|---|---|
| GPT-4o | $2.50/$10.00 | $2.50/$10.00 | **正確** |
| Claude Sonnet 4.5 | $3.00/$15.00 | $3.00/$15.00 | **正確** |
| GPT-4o-mini | $0.15/$0.60 | $0.15/$0.60 | **正確** |
| **Claude Haiku 4.5** | **$0.80/$4.00** | **$1.00/$5.00** | **誤り（旧Haiku 3.5の価格）** |
| Gemini 2.0 Flash | $0.10/$0.40 | $0.10/$0.40 | **正確** |

Claude Haiku 4.5の価格を$0.80/$4.00と記載しているが、これはClaude Haiku 3.5の価格である。Claude Haiku 4.5は$1.00/$5.00（入力/出力それぞれ25%値上げ）。Tier 2モデルとしてのコスト試算に影響するため修正が必要。

- **Gemini 2.0 Pro**: ドキュメントに$1.25/$5.00と記載されているが、Gemini 2.0 Proの正式な一般提供価格の確認が必要。Gemini 2.5系の登場により価格体系が変動している可能性がある
- **Gemini 2.0 Flash Lite**: $0.08/$0.30と記載があるが、Google公式のTier 3モデル価格の最新確認が必要

#### 2.3 プロンプト戦略

**良い点**:
- YAMLテンプレート管理方式はバージョニング・A/Bテストに適しており、実用的
- CCプロンプトの構造（system + user + response_format + json_schema）は明確
- 各モジュール（目標生成、社会認識、発話）の個別プロンプト設計が具体的
- JSON Schema出力形式の指定はLLMの構造化出力を活用する適切なアプローチ

**問題点**:
- **プロンプトの日本語/英語混在**: テンプレート内のプロンプトが日本語で書かれているが、LLMのパフォーマンスは一般に英語プロンプトの方が高い。特にGPT-4oやClaudeの推論精度は英語で最適化されている。エージェントの「発話」は任意の言語でよいが、内部推論（CC、目標生成等）のプロンプトは英語を推奨する、という方針が欲しい
- **Few-shot例の欠如**: プロンプトテンプレートにfew-shotの入出力例が含まれていない。特にCCの意思決定プロンプトでは、期待される出力の具体例があるとLLMの応答品質が大幅に向上する
- **プロンプトの長さ見積もりが不正確**: CCプロンプトのシステムプロンプトを「~800トークン」と見積もっているが、セクション4.2のCC実装クラスの`_build_system_prompt`を見ると、性格情報等を含めても300-400トークン程度。入力全体（システム+ユーザー）の見積もりをより正確に行うべき

---

### 3. 認知コントローラ

#### 3.1 設計の実現可能性

**良い点**:
- `CognitiveController`抽象基底クラスと`LLMCognitiveController`の分離は将来の拡張性（ルールベースCC等）を考慮した良い設計
- `BottleneckInput`データクラスの設計が包括的で、CCが必要とする全情報源をカバーしている
- 状態遷移図（IDLE→COLLECTING→COMPRESSING→DECIDING→BROADCASTING）が明確で、フォールバック（前回決定の再利用）が適切に設計されている

**問題点**:
- **`_validate_and_correct`メソッドの未定義**: CC実装クラスの`process`メソッド内で呼ばれるが、具体的な検証・補正ロジックが記載されていない。LLMの出力が不正なJSON、欠損フィールド、矛盾する指示を含む場合の対処が重要。特に`speech_constraints`と`action_directive`の矛盾検出は一貫性保証の核心部分
- **履歴管理の単純さ**: `self._history`がリストで直近10件を保持する設計だが、CC決定の長期的な傾向（同じ行動の繰り返し、目標の停滞等）を検出するメカニズムがない

#### 3.2 動的間隔制御

**良い点**:
- `CCScheduler`の動的間隔計算は合理的。緊急イベント、会話応答要求、行動認識アラート、システム負荷の4因子による調整は網羅的
- MIN_INTERVAL=1.0秒、MAX_INTERVAL=15.0秒の範囲は論文の「中速」に適切に対応

**問題点**:
- **因子の乗算による過剰短縮**: 緊急イベント（最大0.2倍）、会話応答（0.5倍）、AA アラート（0.6倍）が同時に発生すると`5.0 * 0.2 * 0.5 * 0.6 = 0.3秒`となり、MIN_INTERVAL（1.0秒）にクリップされるが、LLM呼び出しのレイテンシ（1-5秒）を考慮すると実質的に毎回のCC完了直後に次のCCが起動される。これはLLMコストの急激な増大を引き起こす。コスト制約を考慮した上限（例: 1分あたり最大N回）も必要
- **システム負荷の定義が曖昧**: `system_load: float`が何を示すか（CPU使用率？LLM APIキュー長？エージェント数？）が不明確

#### 3.3 既存認知アーキテクチャとの比較

**良い点**:
- LIDA、ACT-R、Soar、CLARIONとの比較表は包括的で正確
- 各アーキテクチャからの再利用可能な設計パターンの整理が実用的
- 参考文献が適切で、2025-2026年の最新研究（GWTマーカー評価、Cognitive Design Patterns for LLM Agents等）を含む

**問題点なし**: この比較セクションは非常に質が高い。

---

### 4. 記憶システム

#### 4.1 三層構造の設計

**良い点**:
- WM/STM/LTMの各層のパラメータ（容量、保持期間、更新頻度、永続化方式）が具体的
- WMのJSONスキーマ設計が詳細で、エージェントの「今この瞬間」の状態を適切にモデル化
- STMの重要度スコアリング（ルールベース + LLMハイブリッド）は効率的でコスト意識が高い
- LTMの記憶種類（エピソード/セマンティック/手続き）の3分類は認知科学的に妥当

**問題点**:
- **WMの容量2,048トークン**: 「LLMのコンテキストウィンドウ内に収まる」が根拠とされているが、CCのプロンプトにはWM以外にも目標、社会的状況、行動認識結果、関連記憶等が含まれる。WM単体でコンテキストの何%を使うかの配分設計が必要。CCプロンプト全体の上限（例: 4,000トークン）を定め、WMはその30%（~1,200トークン）等とすべき
- **STMの容量100件**: 「30分実験で十分」とされるが、4時間の長期実験では1件/10秒のイベント生成で2,400件が発生する。容量超過時のLTM統合が頻繁に発生し、LLMコストが増大する。長時間実験用の容量調整メカニズムが必要
- **LTMの忘却関数のlambdaパラメータ**: `lambda = 0.001`は「2.5時間で約0.4まで減衰」と記載があるが、`exp(-0.001 * 2.5 * 3600) = exp(-9) ≈ 0.00012` である。これは事実上完全に忘却される。正しい計算では`exp(-0.001 * 2.5) = 0.9975`（ほぼ減衰なし）となり、時間の単位（秒 vs 時間）に混乱がある。**数式の時間単位を明確にし、パラメータ値を修正すべき**

#### 4.2 Qdrant選定の妥当性

**ドキュメントの主張**:
- クエリ遅延: ~6ms
- スループット: 2,000 QPS

**Web検索による検証結果**:
- Qdrant公式ベンチマーク等の第三者ベンチマークでは、50Mベクトル @ 99% recallで**41.47 QPS**、SQuADデータセットで**4.70 QPS**（レイテンシ94.52ms）という結果がある
- pgvectorscaleが同条件で471.57 QPS / 1,589 QPSを達成
- Milvusが46.33 QPSで同等性能

**分析**:
ドキュメント記載の6ms/2,000 QPSは**理想的な条件（小規模データ、HNSWインデックス最適化済み、フィルタリングなし）での値**と思われる。実際のPIANO環境（1,000万ベクトル、agent_idフィルタリング、メタデータ付き検索）では、ベンチマーク値は大幅に低下する可能性が高い。

ただし、PIANOの要件（ピーク1,000 QPS、p95 < 50ms）を考慮すると：
- agent_idでのコレクション分割（1エージェント = 1コレクション or パーティション）により、各クエリは最大10,000ベクトルに対してのみ検索すればよい
- この規模であればQdrantの実測性能でも十分対応可能

**結論**: Qdrant選定自体は妥当だが、ベンチマーク数値の根拠を修正し、agent_idベースのパーティショニング戦略を前提とした現実的な性能見積もりに更新すべき。

**追加の検討事項**: pgvectorscale（PostgreSQL拡張）は検索スループットでQdrantを上回るベンチマーク結果が出ている。PIANOでは既にPostgreSQLを使う可能性がある（Mineflayer状態管理等）なら、pgvectorscaleも有力な代替候補として検討に値する。

#### 4.3 先行研究との比較

**良い点**:
- Generative Agents、MemGPTとの比較が詳細で、採用要素と改善点が明確
- 最新研究動向（A-MEM、AriGraph、MIRIX、M2PA）の参照が適切
- 各手法からの借用が具体的（例: Generative Agentsの三要素検索、MemGPTの階層的ページング）

**問題点**:
- **Generative Agentsのスケール**: 「25体」と記載しているが、Generative Agents (Park et al., 2023)は25体のエージェントを使用しているのは正確。ただし、PIANOの論文自体の25体短期実験との対比をもう少し明示するとよい
- **共有記憶層の設計欠如**: 04のセクション4.10で「将来拡張」として共有記憶層を挙げているが、論文の500体文明シミュレーションでは文化的記憶の伝達が重要な要素。Phase 0 MVPでは不要だが、設計上の拡張ポイントをインターフェースレベルで準備しておくべき

---

### 5. コスト最適化

#### 5.1 コスト試算の妥当性

**良い点**:
- モジュール別の呼び出し頻度・トークン数の推定が論文の実験パラメータと整合
- 規模別コスト見積もりの表が明確で、意思決定に有用
- 最適化手法の内訳（モデルミックス50-70%、プロンプトキャッシング50-90%等）が具体的

**問題点**:

1. **バッチ処理設計の根本的矛盾**:
   - OpenAI Batch APIは「50%割引、**24時間以内**に結果返却」であり、PIANOの低速モジュール（目標生成: 60秒間隔、計画: 60秒間隔）の応答時間要件（10秒以内）と根本的に両立しない
   - ドキュメントではBatch APIの50%割引をコスト削減に含めているが、**24時間のレイテンシはリアルタイムシミュレーションでは使用不可能**
   - 正しくは: (a) 独自のマイクロバッチ（複数エージェントのリクエストを数秒間バッファリングして一括送信）によるスループット向上、または (b) Batch APIは事後分析・ログ処理等のオフラインタスクにのみ適用、と明記すべき

2. **最適化後コストの信頼性**:
   - 最適化手法の削減率を単純に乗算している（モデルミックス70% * キャッシング50% * バッチ50% = 82.5%削減）が、これらの手法は独立ではなく相互依存がある
   - 例: モデルミックスでTier 2-3を低コストモデルにした場合、さらにキャッシングで50%削減しても、削減の絶対額は小さくなる
   - 保守的な見積もり（60-70%削減）と楽観的な見積もり（85-90%削減）を分けて提示すべき

3. **セマンティックキャッシュの実用性**:
   - 閾値0.95でのキャッシュは「ほぼ同一のクエリ」のみヒットし、PIANOのような動的環境では実効ヒット率が非常に低い可能性がある
   - GPTCacheの成熟度と運用コスト（ベクトルDB + Redis等の追加インフラ）を考慮すると、Phase 0-1では過剰な最適化。Phase 2以降の検討事項とすべき

#### 5.2 プロンプトキャッシング

**良い点**:
- OpenAI/Anthropicのプロンプトキャッシングの仕様を正確に記載
- PIANOでのキャッシュ対象（システムプロンプト + 性格特性 = ~1,000トークン）の特定が適切

**問題点**:
- **Anthropicプロンプトキャッシングの記載**: 「書き込み1.25倍」は5分TTLの場合に正確。1時間TTLは「書き込み2倍」と記載しているが、最新ドキュメントでも同様の構造で正確
- OpenAIの自動プロンプトキャッシングは1,024トークン以上で発動するが、PIANOのシステムプロンプト（~300-400トークン、上述）だけではこの閾値に達しない可能性がある。性格特性テンプレート等を含めて確実に1,024トークンを超えるプロンプト設計が必要

---

### 6. 実装の詳細度

#### 6.1 コードに落とし込める具体性

**十分に具体的な部分**:
- 03の`CognitiveController`抽象クラス、`LLMCognitiveController`実装クラス、`BottleneckInput`/`CognitiveControllerOutput`データクラス → そのままPythonコードに転写可能
- 04の`MemoryModule` APIインターフェース → 実装の骨格として十分
- 02のプロンプトテンプレート（YAML形式） → プロンプト管理システムに直接使用可能
- 03の`BroadcastManager` → asyncioベースの実装として直接使用可能

**不十分な部分**:

1. **非LLMモジュール（行動認識）の設計が浅い**:
   - 02のセクション5.1で「128次元入力→256→128→出力」のNNアーキテクチャを示しているが、入力ベクトルの構成方法（Minecraftの状態をどう128次元ベクトルに変換するか）が不明
   - 「ルールベースで開始」の代替案が示されているが、具体的なルール体系（何種類のアクション × 何種類の結果パターン）が不明
   - 論文で「最も重要なモジュール」とされているにもかかわらず設計深度が相対的に低い

2. **モジュール間の通信プロトコルが未定義**:
   - 共有エージェント状態（SharedAgentState）への読み書きの同期制御（ロック、バージョニング、競合解決）が未設計
   - 複数モジュールが同時に共有状態を更新する場合の整合性保証メカニズムが必要

3. **エラーハンドリングの不足**:
   - LLMが不正なJSON、スキーマ違反の応答、タイムアウトを返した場合の具体的な対処フロー
   - CCのFALLBACK状態への遷移条件と復帰条件の詳細

4. **テスト戦略の欠如**:
   - CCの意思決定品質をどう評価するか（一貫性スコア、目標達成率等）
   - 記憶システムの検索精度の評価方法
   - 回帰テストの方針

---

## 問題点リスト（優先度付き）

| # | 優先度 | 対象ファイル | 問題の概要 | 推奨対応 |
|---|--------|-------------|-----------|---------|
| 1 | **Critical** | 02 | バッチ処理（OpenAI Batch API 24h）をリアルタイムモジュールのコスト削減に含めている | Batch APIは事後分析用に限定し、リアルタイム用はマイクロバッチ（数秒間のバッファリング）に修正 |
| 2 | **Critical** | 04 | LTM忘却関数のlambdaパラメータと時間単位の矛盾（0.001で2.5時間→0.4と記載だが計算が合わない） | 時間単位を明確化し、パラメータ値を実際の減衰曲線と一致するよう修正 |
| 3 | **High** | 02 | Claude Haiku 4.5の価格が$0.80/$4.00と記載（実際は$1.00/$5.00、旧Haiku 3.5の価格と混同） | 最新価格に更新し、コスト試算を再計算 |
| 4 | **High** | 04 | Qdrantベンチマーク数値（6ms/2,000 QPS）が実測値と大きく乖離 | agent_idパーティショニング前提の現実的性能見積もりに修正 |
| 5 | **High** | 02 | TypeScript/Python混在 | 02のインターフェースをPythonに統一するか、ブリッジ設計を明記 |
| 6 | **High** | 03 | `_validate_and_correct`メソッドの具体的ロジックが未定義 | LLM出力の検証（スキーマ検証、矛盾検出）と補正（デフォルト値挿入、前回出力フォールバック）を設計 |
| 7 | **High** | 02 | 非LLMモジュール（行動認識）の入力ベクトル構成方法が未定義 | Minecraftの状態（インベントリ差分、位置変化等）からの特徴量設計を追加 |
| 8 | **Medium** | 03 | CCスケジューラの因子乗算による過剰短縮とコスト制約の欠如 | 1分あたりの最大実行回数の上限を追加 |
| 9 | **Medium** | 02 | プロンプトの日本語/英語方針が未定義 | 内部推論は英語、エージェント発話は任意言語という方針を明記 |
| 10 | **Medium** | 02 | Few-shot例がプロンプトテンプレートに含まれていない | CCプロンプト等にfew-shot入出力例を追加 |
| 11 | **Medium** | 04 | STM容量100件が長時間実験（4h）で不足する可能性 | シミュレーション時間に応じた動的容量調整メカニズムを追加 |
| 12 | **Medium** | 03 | GWTの「点火（Ignition）」プロセスの明示的実装が欠如 | 情報選択に閾値ベースの動的フィルタリングメカニズムを追加 |
| 13 | **Low** | 02 | セマンティックキャッシュ（GPTCache）のPhase 0での導入は時期尚早 | Phase 2以降の検討事項に移動 |
| 14 | **Low** | 03 | 共有エージェント状態の同時アクセス制御（ロック/バージョニング）が未設計 | Phase 1以降で設計追加 |
| 15 | **Low** | 02 | ストリーミング応答の活用方針がない | 発話モジュールでのストリーミング活用方針を追加 |
| 16 | **Low** | 04 | 共有記憶層の拡張ポイントがインターフェースに準備されていない | MemoryModule APIに将来の共有記憶用メソッドのプレースホルダーを追加 |

---

## 推奨事項

### 即座に対応すべき事項（Phase 0着手前）

1. **Batch API問題の修正**: 02のコスト最適化セクションからBatch APIのリアルタイム適用を削除し、代わりにマイクロバッチング（複数エージェントのリクエストを2-5秒バッファリングして一括送信）の設計に置換する。Batch APIはシミュレーション後のログ分析等に限定する旨を明記する

2. **LTM忘却関数の修正**: lambdaの時間単位を「時間」に統一し、`lambda = 0.5`（半減期 ≈ 1.4時間）に設定する。これはセクション4.6.1の検索recency関数と一致する。あるいは時間単位を「秒」にするなら`lambda = 0.000139`（= 0.5/3600）に修正する

3. **価格情報の最新化**: Claude Haiku 4.5を$1.00/$5.00に修正。Gemini 2.0 Pro、Gemini 2.0 Flash Liteの価格も最新情報で検証する。コスト試算表を再計算する

4. **Qdrant性能見積もりの現実化**: ベンチマーク数値をagent_idパーティショニング前提の現実的値に修正する。例: 「10,000ベクトル/パーティションに対して ~1-5ms、全体スループットはシャード数に比例」

### Phase 0 MVP実装時に考慮すべき事項

5. **TypeScript/Python統一**: 全インターフェースをPythonで記述する。TypeScriptが必要な場合（Mineflayerとの連携等）は、ブリッジ（ZMQ/gRPC/REST）の設計を明示する

6. **行動認識モジュールのルールベース設計**: Minecraftの主要アクション（mine_block, place_block, craft_item, pickup_item, equip, eat, attack, interact）ごとに期待結果と検証ルールのカタログを作成する。NNへの移行は十分なログデータが蓄積されてから

7. **CC出力の検証ロジック**: JSON Schemaバリデーション + 意味的整合性チェック（例: action=flee なのに speech_intent=invite_to_cooperate は矛盾）の基本ルールを設計する

### 中長期的な改善提案

8. **情報ボトルネックの理論的深化**: Tishbyの情報ボトルネック理論に基づくより厳密な情報選択アルゴリズムの検討。現在の3軸評価（緊急性/関連性/新規性）に加えて、「意思決定への情報利得」の推定を加える

9. **CCスケジューラのコスト意識**: 動的間隔制御にコスト予算制約を組み込む。「残りの時間バジェットがX分でコスト上限がY円の場合、CC実行頻度をZ回/分に制限」等のロジック

10. **プロンプトの体系的最適化**: Phase 0以降、実際のLLM応答データを収集し、プロンプトの体系的な最適化（DSPyのような自動プロンプト最適化フレームワークの活用）を検討する
