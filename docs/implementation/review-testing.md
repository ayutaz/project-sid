# テスト戦略レビュー

## 概要

本ドキュメントは、Project Sid再現実装の技術調査ドキュメント（01～10）に対するテスト・品質保証観点のレビュー結果である。10個のモジュールが並列動作し、LLM呼び出しを多用する特殊なシステムに適したテスト戦略を提案する。

---

## 1. テストピラミッド設計

```
                    ┌─────────────┐
                    │   E2E       │  5%  (文明実験再現)
                    │   Tests     │
                 ┌──┴─────────────┴──┐
                 │  Integration      │  20% (モジュール間連携)
                 │  Tests            │
              ┌──┴───────────────────┴──┐
              │  Unit Tests             │  75% (各モジュール単体)
              │  (Module-level)         │
              └─────────────────────────┘
```

### 各層の特徴

| 層 | 比率 | LLM使用 | 実行時間 | 実行頻度 |
|---|---|---|---|---|
| **Unit Tests** | 75% | モック | ~5分 | 毎コミット |
| **Integration Tests** | 20% | モック（一部実API） | ~15分 | 毎PR |
| **E2E Tests** | 5% | 実API | ~1-4時間 | Nightly |

---

## 2. 各モジュールのテスト戦略

### 2.1 テスタビリティ評価

| モジュール | テスタビリティ | 課題 | 推奨テスト手法 |
|---|---|---|---|
| **認知コントローラ(CC)** | ★★★☆☆ | 入力統合の正確性検証が困難 | スナップショットテスト + ゴールデンファイル |
| **記憶システム** | ★★★★☆ | ベクトル検索の品質評価 | Recall@K、MRR指標 |
| **行動認識** | ★★★★★ | NN推論は決定論的 | 入力-出力ペアの回帰テスト |
| **目標生成** | ★★☆☆☆ | LLM出力の非決定性 | 構造検証 + セマンティックテスト |
| **計画** | ★★☆☆☆ | 同上 | 計画の実行可能性検証 |
| **社会認識** | ★★★☆☆ | 感情推定の主観性 | 統計的分布テスト |
| **発話** | ★★☆☆☆ | 自然言語出力の評価 | 構造検証 + LLM-as-judge |
| **スキル実行** | ★★★★☆ | Minecraft環境依存 | モックサーバー + シナリオテスト |
| **自己省察** | ★★☆☆☆ | 出力の質的評価 | ゴールデンファイル比較 |
| **共有エージェント状態** | ★★★★★ | 決定論的データ構造 | CRUD + 並行アクセステスト |

### 2.2 モジュール別テスト戦略詳細

#### 認知コントローラ(CC) — Doc03

| テスト種類 | 内容 | 優先度 |
|---|---|---|
| Unit | 情報圧縮の入出力検証（トークン数制限内か） | P0 |
| Unit | ブロードキャスト配信先の正確性 | P0 |
| Unit | 緊急度判定ロジック | P0 |
| Integration | CC出力→発話モジュールの条件付け | P1 |
| Integration | CC出力→スキル実行モジュールの行動変換 | P1 |

#### 記憶システム — Doc04

| テスト種類 | 内容 | 優先度 |
|---|---|---|
| Unit | WM/STM/LTM各層のCRUD | P0 |
| Unit | 層間遷移ルールの検証 | P0 |
| Unit | ベクトル検索のRecall@K | P0 |
| Integration | 記憶→CC→目標生成のフロー | P1 |
| Performance | 1000エージェント×10,000記憶のクエリ遅延 | P2 |

#### 行動認識 — Doc07

| テスト種類 | 内容 | 優先度 |
|---|---|---|
| Unit | 期待-実際比較の正確性（6種のハルシネーションパターン） | P0 |
| Unit | ループ検出（n-gram分析）の精度 | P0 |
| Unit | 自己修正アクション生成の妥当性 | P1 |
| Regression | 学習データに基づく回帰テスト | P1 |

#### スキル実行 — Doc05

| テスト種類 | 内容 | 優先度 |
|---|---|---|
| Unit | 高レベル意図→低レベルアクション変換 | P0 |
| Integration | Mineflayer API呼び出しの正確性 | P0 |
| E2E | アイテム収集ベンチマーク（30分で17種） | P2 |

---

## 3. LLMテスト戦略

### 3.1 モック戦略

| 層 | 戦略 | ツール |
|---|---|---|
| **Unit** | 固定レスポンスのモックLLM | `unittest.mock` / カスタムMockLLMProvider |
| **Integration** | 記録済みレスポンスの再生 | VCR.py / カスタムキャッシュプロキシ |
| **E2E** | 実API（temperature=0） | 直接呼び出し |

### 3.2 LLMモッククラス設計

```python
class MockLLMProvider:
    def __init__(self, responses: dict[str, str]):
        """モジュール名→固定レスポンスのマッピング"""
        self.responses = responses
        self.call_log = []

    async def complete(self, request):
        module = request.metadata.get("module", "default")
        self.call_log.append(request)
        return CompletionResponse(
            content=self.responses.get(module, "default response"),
            usage=Usage(input_tokens=100, output_tokens=50)
        )
```

### 3.3 プロンプト回帰テスト

| 手法 | 用途 | 頻度 |
|---|---|---|
| **構造検証** | LLM出力がJSON Schemaに準拠するか | 毎コミット |
| **ゴールデンファイル** | 同一入力での出力を過去のベースラインと比較 | 毎PR |
| **セマンティックテスト** | LLM出力の意味的妥当性をLLM-as-judgeで評価 | Nightly |
| **A/Bテスト** | プロンプト変更の効果を統計的に評価 | 手動 |

---

## 4. 再現性の保証

### 4.1 決定論的テスト（Level 1）

| 項目 | 手法 |
|---|---|
| ランダムシード | 全ランダム操作にシード固定（`random.seed()`, `numpy.random.seed()`) |
| LLM出力 | `temperature=0` + キャッシュ再生 |
| Minecraft環境 | 固定ワールドシード + 固定スポーン位置 |
| 時間 | シミュレーションティックベースの仮想時間 |

### 4.2 統計的再現性（Level 2）

| 項目 | 手法 |
|---|---|
| 複数回実行 | 各条件10-20回の繰り返し（論文の4回は不十分） |
| 統計検定 | Mann-Whitney U検定、Kruskal-Wallis検定 |
| 効果量 | Cohen's d / η²の報告 |
| ブートストラップ | 95%信頼区間の算出 |

### 4.3 論文結果の再現検証チェックリスト

| 論文結果 | 再現基準 | テスト方法 |
|---|---|---|
| 30分で平均17種アイテム | 平均15-20種の範囲内 | 25エージェント×5回 |
| 4時間で約320種飽和 | 280-360種の範囲内 | 49エージェント×3回 |
| 社会認知 r=0.807 | r>0.7（同等水準） | 50エージェント×4回 |
| 6種類の役割分化 | 5種類以上 | 30エージェント×4回 |
| 税率20%→9%（反税条件） | 有意な税率低下 | 29エージェント×4回 |

---

## 5. セキュリティテスト戦略

review-securityで特定された15件のリスクに対するテストケースを定義する。セキュリティテストはテストピラミッドの各層に統合する。

### 5.1 セキュリティテストの位置づけ

```
                    ┌─────────────┐
                    │ Penetration │  手動 (Phase 2以降)
                    │   Test      │
                 ┌──┴─────────────┴──┐
                 │ Security          │  Integration層に統合
                 │ Integration Tests │
              ┌──┴───────────────────┴──┐
              │ Security Unit Tests     │  Unit層に統合
              │ + 依存関係スキャン        │
              └─────────────────────────┘
```

### 5.2 セキュリティUnit Tests

| テスト対象 | リスクID | テスト内容 | 実装ツール |
|---|---|---|---|
| プロンプトサニタイザ | C-1 | 100件の既知インジェクションパターンの検出率≧99% | pytest + カスタムフィクスチャ |
| APIキー秘匿性 | C-2 | ログ/エラーメッセージ/レスポンスにAPIキーが含まれないことの検証 | 正規表現 + pytest |
| コスト上限 | C-2 | 月次/日次コスト上限超過時のブロック動作 | MockLLMProvider |
| ループ検出 | H-1 | n-gramベースの同一アクション繰り返し検出（3回以上） | pytest |
| コードサンドボックス | H-3 | 危険な操作（os.system, subprocess, open等）の検出・拒否 | AST分析 + pytest |
| 入力長制限 | C-1 | 過剰に長い入力のトランケーション検証 | pytest |

### 5.3 セキュリティIntegration Tests

| テスト対象 | リスクID | テスト内容 | 実行頻度 |
|---|---|---|---|
| エージェント間インジェクション | C-1 | 悪意あるエージェント発話が他エージェントの行動を変化させないことの検証 | 毎PR |
| Redis ACL | C-3 | 未認証プロセスからのSAS読み書き拒否 | 毎PR |
| 記憶分離 | H-4 | エージェントAの記憶がエージェントBから読み取れないことの検証 | 毎PR |
| LLM API障害フォールバック | H-1 | API タイムアウト時のフォールバック動作 | 毎PR |

### 5.4 依存関係脆弱性スキャン（CI/CD統合）

review-securityのセクション6「セキュリティテスト連携マッピング」の依存関係スキャン定義に準拠し、CI/CDパイプラインに以下を統合する:

- **毎コミット**: シークレットスキャン（gitleaks）
- **毎PR**: Python/Node.js依存関係脆弱性スキャン（pip-audit / npm audit）
- **Weekly**: コンテナイメージスキャン（Trivy）

---

## 6. パフォーマンステスト

### 6.1 負荷テスト計画

| テスト | 対象 | 指標 | 閾値 |
|---|---|---|---|
| **LLM遅延** | LLMゲートウェイ | P95レイテンシ | <5秒 |
| **SAS読み書き** | Redis | P99レイテンシ | <10ms |
| **記憶検索** | Qdrant | P95レイテンシ | <50ms |
| **MCサーバーTPS** | Pufferfish | TPS | >15 (20が理想) |
| **エージェントスループット** | 全体 | モジュール実行回数/秒 | 規模に比例 |

### 6.2 スケーリングテスト

| フェーズ | エージェント数 | 期待結果 |
|---|---|---|
| Smoke | 1体 | 全モジュール正常動作 |
| Small | 10体 | 線形スケーリング |
| Medium | 50体 | 性能劣化<20% |
| Large | 500体 | 安定動作（分散実行） |
| Full | 1000体 | 安定動作（マルチノード） |

---

## 7. デバッグ手法

### 7.1 推奨デバッグツール

| ツール | 用途 | 対象Doc |
|---|---|---|
| **イベントソーシング+リプレイ** | エージェント行動の完全再生 | Doc10 |
| **タイムトラベルデバッグ** | スナップショットからのフォーク実行 | Doc10 |
| **構造化ログ検索** | agent_id, module, trace_idでのフィルタリング | Doc10 |
| **CLIデバッグツール** | リアルタイムのエージェント状態確認 | Doc10 |
| **Webダッシュボード** | 社会グラフ、感情変化の可視化 | Doc09 |

### 7.2 デバッグ困難な領域

| 領域 | 困難な理由 | 推奨アプローチ |
|---|---|---|
| **創発的行動** | 予期しない集団パターンの原因特定 | ログ+社会グラフの遡及分析 |
| **LLM出力品質** | 非決定性+ブラックボックス | ゴールデンファイル+統計的監視 |
| **並行モジュール間の競合** | タイミング依存のバグ | イベントソーシング+決定論的リプレイ |
| **1000体規模のバグ** | ローカル再現不可 | 分散ログ+メトリクスベースの異常検出 |

---

## 8. CI/CDパイプラインの妥当性チェック

### Doc10のCI/CD設計に対するレビュー

| 項目 | 評価 | コメント |
|---|---|---|
| 段階的ゲート設計 | ★★★★★ | lint→unit→integration→E2Eの順は適切 |
| マトリクスビルド | ★★★★☆ | 9モジュール並列は効率的。Python 3.11/3.12の2バージョン対応は妥当 |
| Nightly E2Eテスト | ★★★★☆ | 実LLM APIテストをnightlyに限定する判断は適切 |
| LLMモックの不足 | ★★☆☆☆ | **Warning**: モックLLMの具体的実装が不十分。テストフィクスチャの標準化が必要 |
| 再現性保証 | ★★★☆☆ | Hydra+MLflowは良い。LLMキャッシュによる決定論的テストの記述が不足 |
| パフォーマンスゲート | ★★☆☆☆ | **Warning**: スケーリングテストがCI/CDに組み込まれていない |

### 追加推奨

1. **LLMモックフィクスチャの標準化**: 全モジュールテストで使用するモックレスポンスセットを共通管理
2. **パフォーマンス回帰テスト**: PRごとにベンチマーク実行し、性能劣化を検出
3. **カオステスト**: LLM API障害、MCサーバーダウンのシミュレーション（Phase 2以降）

---

## 9. 優先実装ランキング

| 順位 | テスト | 対象 | 工数 | 効果 |
|---|---|---|---|---|
| 1 | SASのCRUD + 並行アクセスUnit | 01 | 小 | 基盤の安定性保証 |
| 2 | MockLLMProvider | 02 | 小 | 全モジュールテストの基盤 |
| 3 | CC入出力のスナップショットテスト | 03 | 中 | 核心モジュールの品質保証 |
| 4 | 記憶システムのRecall@Kテスト | 04 | 中 | 記憶品質の定量評価 |
| 5 | 行動認識の回帰テスト | 07 | 中 | ハルシネーション検出の信頼性 |
| 6 | Mineflayerモックサーバー | 05 | 大 | MC非依存のテスト環境 |
| 7 | 社会認知の統計的分布テスト | 06 | 中 | 感情追跡の品質保証 |
| 8 | アイテム収集E2E（30分ベンチマーク） | 05, 09 | 大 | 論文再現の基礎検証 |
| 9 | スケーリング負荷テスト | 08 | 大 | スケール対応の保証 |
| 10 | 文明実験E2E（専門化/投票） | 09 | 極大 | 論文の核心結果の再現 |

---

## 10. まとめ

- **テストピラミッド**: Unit 75% / Integration 20% / E2E 5% の比率を推奨
- **LLMテスト**: モックLLM + ゴールデンファイル + LLM-as-judgeの3層構造
- **セキュリティテスト**: review-securityの15件のリスクに対応するテストケースを定義（セクション5）。プロンプトインジェクション検出テスト、依存関係脆弱性スキャン等をCI/CDに統合
- **再現性**: 決定論的テスト（Level 1）と統計的再現性（Level 2）の二重保証
- **最優先**: SAS CRUDテスト、MockLLMProvider、CC入出力テスト、プロンプトサニタイザテストの4点
- **課題**: Doc10のCI/CD設計にLLMモック標準化とパフォーマンスゲートの追加が必要
- **論文再現**: 5つの主要結果に対する再現検証チェックリストを策定
